{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8p4bNxl1iXmAs8xiuAy2A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VpKZoZF2iLj_","executionInfo":{"status":"ok","timestamp":1720276477225,"user_tz":-300,"elapsed":509,"user":{"displayName":"Salman Azeem","userId":"06529966687216548846"}},"outputId":"6ec4f381-c5ff-46b8-a6f2-74c4b8ae3801"},"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/740566/2809126/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240526%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240526T103711Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=53edd6750f08d3384edf001a8d02847331ba144bc002a14f5862fa55f41a77c5c74a2fd3db122d4c8b94ae5224ec3e863875f0742ba791d4c44c7869937d4b91ffe051b6c064c15ae7ebc334e23524668658e3562cb99082709e1e2a598e981dc4b0aa3146c0899940f21dd0bb609dde2c66557a81fc9f9f41210c0c3a6d2760b03ba06ccf8e2c14f26e24060d392f51faf9748e0503d1f76a974ce09d18185b3f7d265c15b4a2540af4ca090cb4e527be091d002afc582bb04c82bcb2750eb634865440f518d914dfce6a6850a368d3c40fb50214a40caf58e8757e4acc99b164298400813d34561f45fed823aed4e69b333d5d6706413df79293d52f60c67b to path /kaggle/input/brain-tumor-detection\n","Data source import complete.\n"]}],"source":["# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'brain-tumor-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F740566%2F2809126%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240526%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240526T103711Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D53edd6750f08d3384edf001a8d02847331ba144bc002a14f5862fa55f41a77c5c74a2fd3db122d4c8b94ae5224ec3e863875f0742ba791d4c44c7869937d4b91ffe051b6c064c15ae7ebc334e23524668658e3562cb99082709e1e2a598e981dc4b0aa3146c0899940f21dd0bb609dde2c66557a81fc9f9f41210c0c3a6d2760b03ba06ccf8e2c14f26e24060d392f51faf9748e0503d1f76a974ce09d18185b3f7d265c15b4a2540af4ca090cb4e527be091d002afc582bb04c82bcb2750eb634865440f518d914dfce6a6850a368d3c40fb50214a40caf58e8757e4acc99b164298400813d34561f45fed823aed4e69b333d5d6706413df79293d52f60c67b'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')"]},{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"id":"gPGjez4giNh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import os\n","import cv2\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Get the path of files\n","image_directory = '../input/brain-tumor-detection/'\n","\n","no_tumor_images = os.listdir(image_directory + 'no/')\n","yes_tumor_images = os.listdir(image_directory + 'yes/')\n","\n","# Initialize dataset and label arrays\n","dataset = []\n","label = []\n","\n","# Set input size\n","INPUT_SIZE = 64\n","\n","# Loop over each image in each category\n","for image_name in no_tumor_images:\n","    if(image_name.split('.')[1] == 'jpg'):\n","        image = cv2.imread(image_directory + 'no/' + image_name)\n","        image = Image.fromarray(image, 'RGB')\n","        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n","        dataset.append(np.array(image))\n","        label.append(0)\n","\n","for image_name in yes_tumor_images:\n","    if(image_name.split('.')[1] == 'jpg'):\n","        image = cv2.imread(image_directory + 'yes/' + image_name)\n","        image = Image.fromarray(image, 'RGB')\n","        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n","        dataset.append(np.array(image))\n","        label.append(1)\n","\n","dataset = np.array(dataset)\n","label = np.array(label)\n","\n","# Split the data\n","x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=42)\n","\n","# Normalize the data\n","x_train = x_train / 255.0\n","x_test = x_test / 255.0\n","\n","# Building a simple CNN for feature extraction\n","feature_extractor = Sequential()\n","feature_extractor.add(Conv2D(32, (3,3), input_shape=(INPUT_SIZE, INPUT_SIZE, 3), activation='relu'))\n","feature_extractor.add(MaxPooling2D(pool_size=(2,2)))\n","feature_extractor.add(Conv2D(32, (3,3), activation='relu'))\n","feature_extractor.add(MaxPooling2D(pool_size=(2,2)))\n","feature_extractor.add(Conv2D(64, (3,3), activation='relu'))\n","feature_extractor.add(MaxPooling2D(pool_size=(2,2)))\n","feature_extractor.add(Flatten())\n","\n","# Extract features using the CNN\n","x_train_features = feature_extractor.predict(x_train)\n","x_test_features = feature_extractor.predict(x_test)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","x_train_features = scaler.fit_transform(x_train_features)\n","x_test_features = scaler.transform(x_test_features)\n","\n","# Train SVM\n","svm = SVC(kernel='rbf')\n","svm.fit(x_train_features, y_train)\n","\n","# Predictions\n","y_pred = svm.predict(x_test_features)\n","\n","# Evaluate\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","\n","# Plot the confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Tumor', 'Tumor'], yticklabels=['No Tumor', 'Tumor'])\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"hEiCIm5eih8_","executionInfo":{"status":"error","timestamp":1720276480705,"user_tz":-300,"elapsed":2846,"user":{"displayName":"Salman Azeem","userId":"06529966687216548846"}},"outputId":"d1c8e08c-3488-4d5c-8b3d-62d3d5f9cc6b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/brain-tumor-detection/no/'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6eac0324f3ed>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimage_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../input/brain-tumor-detection/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mno_tumor_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'no/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0myes_tumor_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'yes/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/brain-tumor-detection/no/'"]}]},{"cell_type":"markdown","source":["# The CODE HAS ENDED HERE .....\n","# BELOW IS PSEUDO CODE (not needed)"],"metadata":{"id":"GG94vE9sXPSk"}},{"cell_type":"code","source":["# Pseudocode for Support Vector Machine (SVM) Algorithm\n","\n","# Input:\n","#   - training_data: a list of tuples where each tuple contains a feature vector and its corresponding label\n","#   - learning_rate: the step size for updating weights\n","#   - epochs: the number of times to iterate over the entire training dataset\n","#   - C: regularization parameter\n","\n","# Output:\n","#   - weights: the weight vector for the SVM\n","#   - bias: the bias term for the SVM\n","\n","function SVM(training_data, learning_rate, epochs, C):\n","    # Initialize weights and bias\n","    weights = [0] * length(training_data[0][0])\n","    bias = 0\n","\n","    # Iterate over the number of epochs\n","    for epoch in range(epochs):\n","        # Iterate over each training example\n","        for feature_vector, label in training_data:\n","            # Calculate the decision function\n","            decision_value = dot_product(weights, feature_vector) + bias\n","\n","            # Check if the example is correctly classified\n","            if label * decision_value >= 1:\n","                # Update weights and bias with regularization\n","                weights = [w - learning_rate * 2 * (1/epochs) * w for w in weights]\n","            else:\n","                # Update weights and bias to correct the misclassification\n","                weights = [w + learning_rate * (label * x - 2 * (1/epochs) * w) for w, x in zip(weights, feature_vector)]\n","                bias += learning_rate * label\n","\n","    return weights, bias\n","\n","# Helper function to calculate the dot product of two vectors\n","function dot_product(vector1, vector2):\n","    return sum([x * y for x, y in zip(vector1, vector2)])\n","\n","# Example usage\n","training_data = [([1, 2], 1), ([2, 3], -1), ([3, 4], 1), ([5, 6], -1)]\n","learning_rate = 0.01\n","epochs = 1000\n","C = 1\n","weights, bias = SVM(training_data, learning_rate, epochs, C)\n","print(weights, bias)  # Output: weight vector and bias term\n"],"metadata":{"id":"Vr8mSFuPl0mL"},"execution_count":null,"outputs":[]}]}