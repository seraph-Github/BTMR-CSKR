{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4xkMNTAS8NVTt5r8+gUz/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JzCUosbNiLRb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VpKZoZF2iLj_","outputId":"1f85fc90-796e-48c3-ae4e-bbef9fe10144","executionInfo":{"status":"ok","timestamp":1720276485118,"user_tz":-300,"elapsed":10,"user":{"displayName":"Salman Azeem","userId":"06529966687216548846"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/740566/2809126/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240526%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240526T103711Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=53edd6750f08d3384edf001a8d02847331ba144bc002a14f5862fa55f41a77c5c74a2fd3db122d4c8b94ae5224ec3e863875f0742ba791d4c44c7869937d4b91ffe051b6c064c15ae7ebc334e23524668658e3562cb99082709e1e2a598e981dc4b0aa3146c0899940f21dd0bb609dde2c66557a81fc9f9f41210c0c3a6d2760b03ba06ccf8e2c14f26e24060d392f51faf9748e0503d1f76a974ce09d18185b3f7d265c15b4a2540af4ca090cb4e527be091d002afc582bb04c82bcb2750eb634865440f518d914dfce6a6850a368d3c40fb50214a40caf58e8757e4acc99b164298400813d34561f45fed823aed4e69b333d5d6706413df79293d52f60c67b to path /kaggle/input/brain-tumor-detection\n","Data source import complete.\n"]}],"source":["# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'brain-tumor-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F740566%2F2809126%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240526%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240526T103711Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D53edd6750f08d3384edf001a8d02847331ba144bc002a14f5862fa55f41a77c5c74a2fd3db122d4c8b94ae5224ec3e863875f0742ba791d4c44c7869937d4b91ffe051b6c064c15ae7ebc334e23524668658e3562cb99082709e1e2a598e981dc4b0aa3146c0899940f21dd0bb609dde2c66557a81fc9f9f41210c0c3a6d2760b03ba06ccf8e2c14f26e24060d392f51faf9748e0503d1f76a974ce09d18185b3f7d265c15b4a2540af4ca090cb4e527be091d002afc582bb04c82bcb2750eb634865440f518d914dfce6a6850a368d3c40fb50214a40caf58e8757e4acc99b164298400813d34561f45fed823aed4e69b333d5d6706413df79293d52f60c67b'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Get the path of files\n","image_directory = '../input/brain-tumor-detection/'\n","\n","no_tumor_images = os.listdir(image_directory + 'no/')\n","yes_tumor_images = os.listdir(image_directory + 'yes/')\n","\n","# Initialize dataset and label arrays\n","dataset = []\n","label = []\n","\n","# Set input size\n","INPUT_SIZE = 64\n","\n","# Loop over each image in each category\n","for image_name in no_tumor_images:\n","    if image_name.split('.')[1] == 'jpg':\n","        image = cv2.imread(image_directory + 'no/' + image_name)\n","        image = Image.fromarray(image, 'RGB')\n","        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n","        dataset.append(np.array(image))\n","        label.append(0)\n","\n","for image_name in yes_tumor_images:\n","    if image_name.split('.')[1] == 'jpg':\n","        image = cv2.imread(image_directory + 'yes/' + image_name)\n","        image = Image.fromarray(image, 'RGB')\n","        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n","        dataset.append(np.array(image))\n","        label.append(1)\n","\n","dataset = np.array(dataset)\n","label = np.array(label)\n","\n","# Split the data\n","x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=42)\n","\n","# Normalize the data\n","x_train = x_train / 255.0\n","x_test = x_test / 255.0\n","\n","# Building a simple CNN for feature extraction\n","feature_extractor = Sequential()\n","feature_extractor.add(Conv2D(32, (3,3), input_shape=(INPUT_SIZE, INPUT_SIZE, 3), activation='relu'))\n","feature_extractor.add(MaxPooling2D(pool_size=(2,2)))\n","feature_extractor.add(Conv2D(32, (3,3), activation='relu'))\n","feature_extractor.add(MaxPooling2D(pool_size=(2,2)))\n","feature_extractor.add(Conv2D(64, (3,3), activation='relu'))\n","feature_extractor.add(MaxPooling2D(pool_size=(2,2)))\n","feature_extractor.add(Flatten())\n","\n","# Extract features using the CNN\n","x_train_features = feature_extractor.predict(x_train)\n","x_test_features = feature_extractor.predict(x_test)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","x_train_features = scaler.fit_transform(x_train_features)\n","x_test_features = scaler.transform(x_test_features)\n","\n","# Train Random Forest\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(x_train_features, y_train)\n","\n","# Predictions\n","y_pred = rf.predict(x_test_features)\n","\n","# Evaluate\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","\n","# Compute the confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(f'Confusion Matrix:\\n{conf_matrix}')\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Tumor', 'Tumor'], yticklabels=['No Tumor', 'Tumor'])\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"LJvPS67SiYtr","executionInfo":{"status":"error","timestamp":1720276499490,"user_tz":-300,"elapsed":14378,"user":{"displayName":"Salman Azeem","userId":"06529966687216548846"}},"outputId":"75b3f58c-3632-4c0b-b37a-722b70e9e217"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/brain-tumor-detection/no/'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1c7b548a4289>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimage_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../input/brain-tumor-detection/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mno_tumor_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'no/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0myes_tumor_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'yes/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/brain-tumor-detection/no/'"]}]},{"cell_type":"markdown","source":["# The CODE HAS ENDED HERE .....\n","# BELOW IS PSEUDO CODE (not needed)"],"metadata":{"id":"NohzzC2cXSLb"}},{"cell_type":"code","source":["# Pseudocode for Random Forest Algorithm\n","\n","# Input:\n","#   - training_data: a list of tuples where each tuple contains a feature vector and its corresponding label\n","#   - num_trees: the number of trees in the forest\n","#   - max_depth: the maximum depth of each tree\n","#   - min_size: the minimum size of a node to split\n","\n","# Output:\n","#   - forest: a list of decision trees\n","\n","function RandomForest(training_data, num_trees, max_depth, min_size):\n","    # Helper function to split a dataset based on an attribute and an attribute value\n","    function test_split(index, value, dataset):\n","        left, right = [], []\n","        for row in dataset:\n","            if row[index] < value:\n","                left.append(row)\n","            else:\n","                right.append(row)\n","        return left, right\n","\n","    # Helper function to calculate the Gini index for a split dataset\n","    function gini_index(groups, classes):\n","        n_instances = float(sum([len(group) for group in groups]))\n","        gini = 0.0\n","        for group in groups:\n","            size = float(len(group))\n","            if size == 0:\n","                continue\n","            score = 0.0\n","            for class_val in classes:\n","                proportion = [row[-1] for row in group].count(class_val) / size\n","                score += proportion * proportion\n","            gini += (1.0 - score) * (size / n_instances)\n","        return gini\n","\n","    # Helper function to select the best split point for a dataset\n","    function get_split(dataset):\n","        class_values = list(set(row[-1] for row in dataset))\n","        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n","        for index in range(len(dataset[0]) - 1):\n","            for row in dataset:\n","                groups = test_split(index, row[index], dataset)\n","                gini = gini_index(groups, class_values)\n","                if gini < b_score:\n","                    b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n","        return {'index': b_index, 'value': b_value, 'groups': b_groups}\n","\n","    # Recursive function to build a decision tree\n","    function build_tree(train, max_depth, min_size, depth):\n","        root = get_split(train)\n","        split(root, max_depth, min_size, depth)\n","        return root\n","\n","    # Recursive function to create child splits for a node or make terminal\n","    function split(node, max_depth, min_size, depth):\n","        left, right = node['groups']\n","        del(node['groups'])\n","        if not left or not right:\n","            node['left'] = node['right'] = to_terminal(left + right)\n","            return\n","        if depth >= max_depth:\n","            node['left'], node['right'] = to_terminal(left), to_terminal(right)\n","            return\n","        if len(left) <= min_size:\n","            node['left'] = to_terminal(left)\n","        else:\n","            node['left'] = get_split(left)\n","            split(node['left'], max_depth, min_size, depth + 1)\n","        if len(right) <= min_size:\n","            node['right'] = to_terminal(right)\n","        else:\n","            node['right'] = get_split(right)\n","            split(node['right'], max_depth, min_size, depth + 1)\n","\n","    # Helper function to create a terminal node value\n","    function to_terminal(group):\n","        outcomes = [row[-1] for row in group]\n","        return max(set(outcomes), key=outcomes.count)\n","\n","    # Helper function to make a prediction with a decision tree\n","    function predict(node, row):\n","        if row[node['index']] < node['value']:\n","            if isinstance(node['left'], dict):\n","                return predict(node['left'], row)\n","            else:\n","                return node['left']\n","        else:\n","            if isinstance(node['right'], dict):\n","                return predict(node['right'], row)\n","            else:\n","                return node['right']\n","\n","    # Function to create a random subsample from the dataset with replacement\n","    function subsample(dataset, ratio):\n","        sample = []\n","        n_sample = round(len(dataset) * ratio)\n","        while len(sample) < n_sample:\n","            index = randrange(len(dataset))\n","            sample.append(dataset[index])\n","        return sample\n","\n","    # Function to build a forest of trees\n","    forest = []\n","    for i in range(num_trees):\n","        sample = subsample(training_data, 1.0)\n","        tree = build_tree(sample, max_depth, min_size, 1)\n","        forest.append(tree)\n","    return forest\n","\n","# Function to make a prediction with a list of bagged trees\n","function random_forest_predict(forest, row):\n","    predictions = [predict(tree, row) for tree in forest]\n","    return max(set(predictions), key=predictions.count)\n","\n","# Example usage\n","training_data = [\n","    [2.771244718, 1.784783929, 0],\n","    [1.728571309, 1.169761413, 0],\n","    [3.678319846, 2.81281357, 0],\n","    [3.961043357, 2.61995032, 0],\n","    [2.999208922, 2.209014212, 0],\n","    [7.497545867, 3.162953546, 1],\n","    [9.00220326, 3.339047188, 1],\n","    [7.444542326, 0.476683375, 1],\n","    [10.12493903, 3.234550982, 1],\n","    [6.642287351, 3.319983761, 1]\n","]\n","num_trees = 5\n","max_depth = 10\n","min_size = 1\n","forest = RandomForest(training_data, num_trees, max_depth, min_size)\n","for row in training_data:\n","    prediction = random_forest_predict(forest, row)\n","    print(f'Expected={row[-1]}, Predicted={prediction}')\n"],"metadata":{"id":"S_TxKhtGi3dr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let\n","𝐷\n","=\n","{\n","(\n","𝑋\n","𝑖\n",",\n","𝑌\n","𝑖\n",")\n","}\n","𝑖\n","=\n","1\n","𝑁\n","𝐷\n","D={(X\n","i\n","​\n"," ,Y\n","i\n","​\n"," )}\n","i=1\n","N\n","D\n","​\n","\n","​\n","  be the learning data set with\n","𝑁\n","𝐷\n","N\n","D\n","​\n","  independent samples. Each sample\n","(\n","𝑋\n","𝑖\n",",\n","𝑌\n","𝑖\n",")\n","(X\n","i\n","​\n"," ,Y\n","i\n","​\n"," ) follows the same unknown joint distribution\n","(\n","𝑋\n",",\n","𝑌\n",")\n","∼\n","𝐹\n","(\n","𝑋\n",",\n","𝑌\n",")\n","(X,Y)∼F(X,Y), where\n","𝑋\n","∈\n","𝑋\n","⊂\n","𝑅\n","𝑑\n","X∈X⊂R\n","d\n","  and\n","𝑌\n","∈\n","𝑌\n","=\n","{\n","𝜗\n","0\n",",\n","𝜗\n","1\n",",\n","𝜗\n","2\n",",\n",".\n",".\n",".\n",",\n","𝜗\n","∣\n","𝑌\n","∣\n","−\n","1\n","}\n","Y∈Y={ϑ\n","0\n","​\n"," ,ϑ\n","1\n","​\n"," ,ϑ\n","2\n","​\n"," ,...,ϑ\n","∣Y∣−1\n","​\n"," }. For simplicity, assume a binary classification task, i.e.,\n","∣\n","𝑌\n","∣\n","=\n","2\n","∣Y∣=2, where\n","𝑌\n","=\n","{\n","𝜗\n","0\n",",\n","𝜗\n","1\n","}\n","Y={ϑ\n","0\n","​\n"," ,ϑ\n","1\n","​\n"," }.\n","\n","The goal is to learn a classifier model\n","ℎ\n",":\n","𝑋\n","→\n","𝑌\n","h:X→Y using the dataset\n","𝐷\n","D. For a new input point\n","𝑥\n","∈\n","𝑋\n","x∈X, the classifier returns a prediction\n","𝑦\n","^\n","=\n","ℎ\n","(\n","𝑥\n",")\n","∈\n","𝑌\n","y\n","^\n","​\n"," =h(x)∈Y. We can associate the value 0 to\n","𝜗\n","0\n","ϑ\n","0\n","​\n","  and the value 1 to\n","𝜗\n","1\n","ϑ\n","1\n","​\n"," . Thus,\n","𝑦\n","^\n","=\n","ℎ\n","(\n","𝑥\n",")\n","=\n","𝐼\n","(\n","𝑔\n","(\n","𝑥\n",")\n",">\n","𝛾\n",")\n","∈\n","{\n","0\n",",\n","1\n","}\n","y\n","^\n","​\n"," =h(x)=I(g(x)>γ)∈{0,1}, where\n","𝐼\n","I is the indicator function,\n","𝑔\n",":\n","𝑋\n","→\n","[\n","0\n",",\n","1\n","]\n","⊂\n","𝑅\n","g:X→[0,1]⊂R is a model outputting real values, and\n","𝛾\n","∈\n","[\n","0\n",",\n","1\n","]\n","γ∈[0,1] is a threshold.\n","\n","Let\n","𝑇\n","=\n","{\n","(\n","𝑋\n","𝑖\n",",\n","𝑌\n","𝑖\n",")\n","}\n","𝑖\n","=\n","1\n","𝑁\n","𝑇\n","T={(X\n","i\n","​\n"," ,Y\n","i\n","​\n"," )}\n","i=1\n","N\n","T\n","​\n","\n","​\n","  be a testing data set with\n","𝑁\n","𝑇\n","N\n","T\n","​\n","  independent samples following the same unknown distribution\n","𝐹\n","(\n","𝑋\n",",\n","𝑌\n",")\n","F(X,Y). To quantify the quality of the predictions made by\n","ℎ\n","h on the samples in\n","𝑇\n","T, we define the following loss function\n","𝐿\n",":\n","𝑌\n","×\n","𝑌\n","→\n","{\n","TP\n",",\n","TN\n",",\n","FP\n",",\n","FN\n","}\n","L:Y×Y→{TP,TN,FP,FN}:\n","\n","𝐿\n","(\n","𝑦\n",",\n","𝑦\n","^\n",")\n","=\n","TP\n","L(y,\n","y\n","^\n","​\n"," )=TP if\n","𝑦\n","=\n","𝜗\n","1\n","y=ϑ\n","1\n","​\n","  and\n","𝑦\n","^\n","=\n","𝜗\n","1\n","y\n","^\n","​\n"," =ϑ\n","1\n","​\n","\n","𝐿\n","(\n","𝑦\n",",\n","𝑦\n","^\n",")\n","=\n","TN\n","L(y,\n","y\n","^\n","​\n"," )=TN if\n","𝑦\n","=\n","𝜗\n","0\n","y=ϑ\n","0\n","​\n","  and\n","𝑦\n","^\n","=\n","𝜗\n","0\n","y\n","^\n","​\n"," =ϑ\n","0\n","​\n","\n","𝐿\n","(\n","𝑦\n",",\n","𝑦\n","^\n",")\n","=\n","FP\n","L(y,\n","y\n","^\n","​\n"," )=FP if\n","𝑦\n","=\n","𝜗\n","0\n","y=ϑ\n","0\n","​\n","  and\n","𝑦\n","^\n","=\n","𝜗\n","1\n","y\n","^\n","​\n"," =ϑ\n","1\n","​\n","\n","𝐿\n","(\n","𝑦\n",",\n","𝑦\n","^\n",")\n","=\n","FN\n","L(y,\n","y\n","^\n","​\n"," )=FN if\n","𝑦\n","=\n","𝜗\n","1\n","y=ϑ\n","1\n","​\n","  and\n","𝑦\n","^\n","=\n","𝜗\n","0\n","y\n","^\n","​\n"," =ϑ\n","0\n","​\n","\n","Let\n","𝐿\n","𝑖\n","=\n","𝐿\n","(\n","𝑌\n","𝑖\n",",\n","𝑌\n","^\n","𝑖\n",")\n","L\n","i\n","​\n"," =L(Y\n","i\n","​\n"," ,\n","Y\n","^\n","  \n","i\n","​\n"," ). The vector\n","𝑀\n","=\n","(\n","𝐿\n","1\n",",\n",".\n",".\n",".\n",",\n","𝐿\n","𝑁\n","𝑇\n",")\n","M=(L\n","1\n","​\n"," ,...,L\n","N\n","T\n","​\n","\n","​\n"," ) contains the values of the loss function computed on the testing set. We count in the vector\n","𝑉\n","=\n","(\n","#\n","TP\n",",\n","#\n","TN\n",",\n","#\n","FP\n",",\n","#\n","FN\n",")\n","∈\n","𝑁\n","4\n","V=(#TP,#TN,#FP,#FN)∈N\n","4\n","  the number of times we obtain from\n","𝐿\n","L a TP, TN, FP, or FN, where:\n","\n","#\n","TP\n","=\n","∑\n","𝑖\n","=\n","1\n","𝑁\n","𝑇\n","𝐼\n","(\n","𝐿\n","𝑖\n","=\n","TP\n",")\n",",\n","#\n","TN\n","=\n","∑\n","𝑖\n","=\n","1\n","𝑁\n","𝑇\n","𝐼\n","(\n","𝐿\n","𝑖\n","=\n","TN\n",")\n",",\n","#\n","FP\n","=\n","∑\n","𝑖\n","=\n","1\n","𝑁\n","𝑇\n","𝐼\n","(\n","𝐿\n","𝑖\n","=\n","FP\n",")\n",",\n","#\n","FN\n","=\n","∑\n","𝑖\n","=\n","1\n","𝑁\n","𝑇\n","𝐼\n","(\n","𝐿\n","𝑖\n","=\n","FN\n",")\n","#TP=\n","i=1\n","∑\n","N\n","T\n","​\n","\n","​\n"," I(L\n","i\n","​\n"," =TP),#TN=\n","i=1\n","∑\n","N\n","T\n","​\n","\n","​\n"," I(L\n","i\n","​\n"," =TN),#FP=\n","i=1\n","∑\n","N\n","T\n","​\n","\n","​\n"," I(L\n","i\n","​\n"," =FP),#FN=\n","i=1\n","∑\n","N\n","T\n","​\n","\n","​\n"," I(L\n","i\n","​\n"," =FN)\n","By definition, the sum of the elements in\n","𝑉\n","V equals\n","𝑁\n","𝑇\n","N\n","T\n","​\n"," :\n","\n","#\n","TP\n","+\n","#\n","TN\n","+\n","#\n","FP\n","+\n","#\n","FN\n","=\n","𝑁\n","𝑇\n","#TP+#TN+#FP+#FN=N\n","T\n","​\n","\n","Given a classifier\n","ℎ\n","h and its results on\n","𝑇\n","T, the vector\n","𝑉\n","V contains all the raw information about the classifier’s predictions on the testing set. This vector is commonly presented in a 2 × 2 matrix form, known as the confusion matrix\n","𝐶\n","C:\n","\n","𝐶\n","=\n","(\n","#\n","TN\n","#\n","FP\n","#\n","FN\n","#\n","TP\n",")\n","C=(\n","#TN\n","#FN\n","​\n","  \n","#FP\n","#TP\n","​\n"," )\n","This matrix effectively encapsulates the performance of the classifier by displaying the counts of true positives, true negatives, false positives, and false negatives."],"metadata":{"id":"ATyonKifRSUP"}}]}